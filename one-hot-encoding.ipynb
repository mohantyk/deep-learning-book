{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat', 'The dog ate the homework', 'The flight to Denver has a cat and a dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The:1\n",
      "cat:2\n",
      "sat:3\n",
      "on:4\n",
      "the:5\n",
      "mat:6\n",
      "dog:7\n",
      "ate:8\n",
      "homework:9\n",
      "flight:10\n",
      "to:11\n",
      "Denver:12\n",
      "has:13\n",
      "a:14\n",
      "and:15\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary of tokens and token indices\n",
    "tokens = {}\n",
    "for sentence in samples:\n",
    "    for word in sentence.split():\n",
    "        if word not in tokens:\n",
    "            tokens[word] = len(tokens) + 1 # The all zero vector is for words not in dictionary\n",
    "            print('{word}:{idx}'.format(word=word, idx=tokens[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode\n",
    "max_words = 20  #Only encode this many words of each sentence\n",
    "\n",
    "one_hot_encoding = np.zeros(shape=(len(samples), max_words, len(tokens) + 1))\n",
    "for sentence_idx, sentence in enumerate(samples):\n",
    "    for word_idx, word in enumerate(sentence.split()[:max_words]):\n",
    "        one_hot_encoding[sentence_idx, word_idx, tokens[word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_token = {idx:token for token, idx in tokens.items()}\n",
    "idx_to_token[0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat on the mat\n",
      "The dog ate the homework\n",
      "The flight to Denver has a cat and a dog\n"
     ]
    }
   ],
   "source": [
    "#Convert from one-hot encoding to sentence\n",
    "for sentence_encoding in one_hot_encoding:\n",
    "    sentence_list = []\n",
    "    for word_encoding in sentence_encoding: \n",
    "        try:\n",
    "            word_idx = word_encoding.nonzero()[0][0]\n",
    "            sentence_list.append(idx_to_token[word_idx])\n",
    "        except IndexError:\n",
    "            # zero-vector not really useful.\n",
    "            pass\n",
    "    print(' '.join(sentence_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras for One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 20) #Create tokenizer with 1000 words\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 5, 6, 1, 7], [1, 3, 8, 1, 9], [1, 10, 11, 12, 13, 4, 2, 14, 4, 3]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text into list of indices\n",
    "tokenizer.texts_to_sequences(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding\n",
    "tokenizer.texts_to_matrix(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 4,\n",
       " 'and': 14,\n",
       " 'ate': 8,\n",
       " 'cat': 2,\n",
       " 'denver': 12,\n",
       " 'dog': 3,\n",
       " 'flight': 10,\n",
       " 'has': 13,\n",
       " 'homework': 9,\n",
       " 'mat': 7,\n",
       " 'on': 6,\n",
       " 'sat': 5,\n",
       " 'the': 1,\n",
       " 'to': 11}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
